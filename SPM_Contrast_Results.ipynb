{
 "metadata": {
  "name": "SPM_Contrast_Results"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import prov.model as prov\n",
      "from uuid import uuid1\n",
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SPM Results Metadata\n",
      "\n",
      "#temporarily store this data in local variables. This will be replaced by parsing SPM.mat, xCon.mat, etc...\n",
      "spm_metadata = {'hthresh_t':3.10, \n",
      "                'hthresh_p':0.001,\n",
      "                'ext_thresh_vx':0,\n",
      "                'ext_thresh_p':1.0,\n",
      "                'exp_vox_cluster':60.632,\n",
      "                'exp_num_clusters':9.51,\n",
      "                'dof':[1.0, 1027.0],\n",
      "                'fwhm_mm':[15.8, 15.8, 16.6],\n",
      "                'fwhm_voxels':[7.9, 7.9, 8.3],\n",
      "                'vol_vox':504268,\n",
      "                'vol_res':938.3,\n",
      "                'vox_size':[2.0, 2.0, 2.0]}\n",
      "\n",
      "\n",
      "\n",
      "#encoding results table as a dictionary of dictionaries\n",
      "#keys could be changed from the ones I'm using here to ones that use the terms from our terminology (possibly)\n",
      "spm_results_table={0: dict(set_level_p=0.836,set_level_c=7,cluster_level_FWEcorr_p=0.831,cluster_level_FDRcorr_q=0.655, cluster_level_Ke=99, \n",
      "                           cluster_level_Uncorr_p=0.187, peak_level_FWEcorr_p=0.728, peak_level_FDRcorr_p=0.464, peak_level_T=3.80, \n",
      "                           peak_level_Z=3.79, peak_level_Uncorr_p=0.000, loc=\"[48, 42,0]\"),\n",
      "                   1: dict(set_level_p=\"[]\",set_level_c=\"[]\",cluster_level_FWEcorr_p=\"[]\",cluster_level_FDRcorr_q=\"[]\", cluster_level_Ke=\"[]\", \n",
      "                           cluster_level_Uncorr_p=\"\",peak_level_FWEcorr_p=0.994, peak_level_FDRcorr_p=0.904, peak_level_T=3.34, \n",
      "                           peak_level_Z=3.33, peak_level_Uncorr_p=0.000, loc=\"[54, 36,-12]\"),\n",
      "                   2: dict(set_level_p=\"[]\",set_level_c=\"[]\",cluster_level_FWEcorr_p=0.145,cluster_level_FDRcorr_q=0.115, cluster_level_Ke=380, \n",
      "                           cluster_level_Uncorr_p=0.016, peak_level_FWEcorr_p=0.756, peak_level_FDRcorr_p=0.464, peak_level_T=3.78, \n",
      "                           peak_level_Z=3.76, peak_level_Uncorr_p=0.000, loc=\"[-20, 52,32]\"),\n",
      "                   3: dict(set_level_p=\"[]\",set_level_c=\"[]\",cluster_level_FWEcorr_p=\"[]\",cluster_level_FDRcorr_q=\"[]\", cluster_level_Ke=\"[]\", \n",
      "                           cluster_level_Uncorr_p=\"[]\",peak_level_FWEcorr_p=0.994, peak_level_FDRcorr_p=0.904, peak_level_T=3.34, \n",
      "                           peak_level_Z=3.33, peak_level_Uncorr_p=0.000, loc=\"[54, 36,-12]\"),\n",
      "                   4: dict(set_level_p=\"[]\",set_level_c=\"[]\",cluster_level_FWEcorr_p=0.998,cluster_level_FDRcorr_q=0.925, cluster_level_Ke=12, \n",
      "                           cluster_level_Uncorr_p=0.663, peak_level_FWEcorr_p=0.997, peak_level_FDRcorr_p=0.904, peak_level_T=3.30, \n",
      "                           peak_level_Z=3.29, peak_level_Uncorr_p=0.001, loc=\"[38, 52,-30]\")}\n",
      "                   \n",
      "    \n",
      "#sample query of spm_results_table for results with large t-score\n",
      "large_t=set(dat for dat in spm_results_table if spm_results_table[dat]['peak_level_T'] > 3.00)\n",
      "#print results from query\n",
      "print 'example query (t-stats > 3.0): set(dat for dat in spm_results_table if spm_results_table[dat][''peak_level_T''] > 3.00)\\n'\n",
      "for i in large_t:\n",
      "    print spm_results_table[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "example query (t-stats > 3.0): set(dat for dat in spm_results_table if spm_results_table[dat][peak_level_T] > 3.00)\n",
        "\n",
        "{'loc': '[48, 42,0]', 'cluster_level_FDRcorr_q': 0.655, 'cluster_level_Ke': 99, 'peak_level_Uncorr_p': 0.0, 'peak_level_FWEcorr_p': 0.728, 'peak_level_FDRcorr_p': 0.464, 'set_level_c': 7, 'peak_level_T': 3.8, 'cluster_level_Uncorr_p': 0.187, 'peak_level_Z': 3.79, 'cluster_level_FWEcorr_p': 0.831, 'set_level_p': 0.836}\n",
        "{'loc': '[54, 36,-12]', 'cluster_level_FDRcorr_q': '[]', 'cluster_level_Ke': '[]', 'peak_level_Uncorr_p': 0.0, 'peak_level_FWEcorr_p': 0.994, 'peak_level_FDRcorr_p': 0.904, 'set_level_c': '[]', 'peak_level_T': 3.34, 'cluster_level_Uncorr_p': '', 'peak_level_Z': 3.33, 'cluster_level_FWEcorr_p': '[]', 'set_level_p': '[]'}\n",
        "{'loc': '[-20, 52,32]', 'cluster_level_FDRcorr_q': 0.115, 'cluster_level_Ke': 380, 'peak_level_Uncorr_p': 0.0, 'peak_level_FWEcorr_p': 0.756, 'peak_level_FDRcorr_p': 0.464, 'set_level_c': '[]', 'peak_level_T': 3.78, 'cluster_level_Uncorr_p': 0.016, 'peak_level_Z': 3.76, 'cluster_level_FWEcorr_p': 0.145, 'set_level_p': '[]'}\n",
        "{'loc': '[54, 36,-12]', 'cluster_level_FDRcorr_q': '[]', 'cluster_level_Ke': '[]', 'peak_level_Uncorr_p': 0.0, 'peak_level_FWEcorr_p': 0.994, 'peak_level_FDRcorr_p': 0.904, 'set_level_c': '[]', 'peak_level_T': 3.34, 'cluster_level_Uncorr_p': '[]', 'peak_level_Z': 3.33, 'cluster_level_FWEcorr_p': '[]', 'set_level_p': '[]'}\n",
        "{'loc': '[38, 52,-30]', 'cluster_level_FDRcorr_q': 0.925, 'cluster_level_Ke': 12, 'peak_level_Uncorr_p': 0.001, 'peak_level_FWEcorr_p': 0.997, 'peak_level_FDRcorr_p': 0.904, 'set_level_c': '[]', 'peak_level_T': 3.3, 'cluster_level_Uncorr_p': 0.663, 'peak_level_Z': 3.29, 'cluster_level_FWEcorr_p': 0.998, 'set_level_p': '[]'}\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# create a prov bundle to store the graph\n",
      "g = prov.ProvBundle()\n",
      "\n",
      "# define namespaces \n",
      "nidm_ns = prov.Namespace(\"nidm\", \"http://nidm.nidash.org/\")\n",
      "spm_ns = prov.Namespace(\"spm\", \"www.fil.ion.ucl.ac.uk/spm/\")\n",
      "neurolex_ns = prov.Namespace(\"neurolex\", \"http://neurolex.org/wiki/Main_Page/\")\n",
      "prov_ns = prov.Namespace(\"prov\", \"http://www.w3.org/ns/prov#\")\n",
      "\n",
      "# add namespaces to the graph\n",
      "g.add_namespace(nidm_ns)\n",
      "g.add_namespace(spm_ns)\n",
      "g.add_namespace(prov_ns)\n",
      "\n",
      "                \n",
      "\n",
      "# Height Threshold\n",
      "# create entity\n",
      "height_threshold_param_id=[0]*2\n",
      "height_threshold_param_id[0]=g.entity(\"height_threshold_param1\", [(prov.PROV[\"type\"],spm_ns[\"height_threshold\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"t-statistic\"]),\n",
      "                         (prov.PROV[\"label\"],\"Height Threshold\"),\n",
      "                         (prov.PROV[\"value\"],spm_metadata['hthresh_t'])]).get_identifier()\n",
      "height_threshold_param_id[1]=g.entity(\"height_threshold_param2\", [(prov.PROV[\"type\"],spm_ns[\"height_threshold\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"uncorrected_statistic\"]),\n",
      "                         (prov.PROV[\"label\"],\"Height_Threshold\"),\n",
      "                         (prov.PROV[\"value\"],spm_metadata['hthresh_p'])]).get_identifier()\n",
      "\n",
      "# create a height threshold collection\n",
      "height_threshold_collection_id=g.collection(\"height_threshold_collection\",[(prov.PROV['type'],spm_ns['Height_Threshold'])]).get_identifier()\n",
      "g.hadMember(height_threshold_collection_id,height_threshold_param_id[0])\n",
      "g.hadMember(height_threshold_collection_id,height_threshold_param_id[1])\n",
      "\n",
      "# Extent Threshold\n",
      "# create entity\n",
      "extent_threshold_param_id=[0]*2\n",
      "extent_threshold_param_id[0]=g.entity(\"extent_threshold_param1\", [(prov.PROV[\"type\"],spm_ns[\"extent_threshold\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"threhold\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"voxel\"]),\n",
      "                         (prov.PROV[\"label\"],\"Extent Threshold\"),\n",
      "                         (prov.PROV[\"value\"],spm_metadata['ext_thresh_vx'])]).get_identifier()\n",
      "extent_threshold_param_id[1]=g.entity(\"extent_threshold_param2\", [(prov.PROV[\"type\"],spm_ns[\"extent_threshold\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"uncorrected_statistic\"]),\n",
      "                         (prov.PROV[\"label\"],\"Extent_Threshold\"),\n",
      "                         (prov.PROV[\"value\"],spm_metadata['ext_thresh_p'])]).get_identifier()\n",
      "\n",
      "# create a extent threshold collection\n",
      "extent_threshold_collection_id=g.collection(\"extent_threshold_collection\",[(prov.PROV['type'],spm_ns['Extent_Threshold'])]).get_identifier()\n",
      "g.hadMember(extent_threshold_collection_id,extent_threshold_param_id[0])\n",
      "g.hadMember(extent_threshold_collection_id, extent_threshold_param_id[1])\n",
      "\n",
      "\n",
      "\n",
      "# Expected voxels per cluster\n",
      "# create entity\n",
      "expected_voxels_id=g.entity(\"expected_voxels\", [(prov.PROV[\"type\"],spm_ns[\"expected_voxels\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"VolumeInVoxels\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"voxel\"]),\n",
      "                         (prov.PROV[\"label\"],\"Expected_voxels_per_cluster\"),\n",
      "                         (prov.PROV[\"value\"],spm_metadata['exp_vox_cluster'])]).get_identifier()\n",
      "\n",
      "\n",
      "\n",
      "# Expected clusters\n",
      "# create entity\n",
      "expected_clusters_id=g.entity(\"expected_clusters\", [(prov.PROV[\"type\"],spm_ns[\"expected_clusters\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"cluster\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"voxel\"]),\n",
      "                         (prov.PROV[\"label\"],\"Expected_number_of_clusters\"),\n",
      "                         (prov.PROV[\"value\"],spm_metadata['exp_num_clusters'])]).get_identifier()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "metadata_collection=g.collection(\"spm_results_metadata\", [(prov.PROV['type'],spm_ns['spm_results_metadata'])]).get_identifier()\n",
      "g.hadMember(metadata_collection,height_threshold_collection_id) \n",
      "g.hadMember(metadata_collection,extent_threshold_collection_id)\n",
      "g.hadMember(metadata_collection,expected_voxels_id)\n",
      "g.hadMember(metadata_collection,expected_clusters_id)\n",
      "\n",
      "\n",
      "print g.get_provn()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "document\n",
        "  prefix neurolex <http://neurolex.org/wiki/Main_Page/>\n",
        "  prefix spm <www.fil.ion.ucl.ac.uk/spm/>\n",
        "  prefix nidm <http://nidm.nidash.org/>\n",
        "  \n",
        "  entity(height_threshold_param1, [prov:type='spm:height_threshold', prov:type='neurolex:t-statistic', prov:label=\"Height Threshold\", prov:value=\"3.100000\" %% xsd:float])\n",
        "  entity(height_threshold_param2, [prov:type='spm:height_threshold', prov:type='neurolex:p-value', prov:type='neurolex:uncorrected_statistic', prov:label=\"Height_Threshold\", prov:value=\"0.001000\" %% xsd:float])\n",
        "  entity(height_threshold_collection, [prov:type='spm:Height_Threshold', prov:type='prov:Collection'])\n",
        "  hadMember(height_threshold_collection, height_threshold_param1)\n",
        "  hadMember(height_threshold_collection, height_threshold_param2)\n",
        "  entity(extent_threshold_param1, [prov:type='spm:extent_threshold', prov:type='neurolex:threhold', prov:type='neurolex:voxel', prov:label=\"Extent Threshold\", prov:value=0])\n",
        "  entity(extent_threshold_param2, [prov:type='spm:extent_threshold', prov:type='neurolex:p-value', prov:type='neurolex:uncorrected_statistic', prov:label=\"Extent_Threshold\", prov:value=\"1.000000\" %% xsd:float])\n",
        "  entity(extent_threshold_collection, [prov:type='spm:Extent_Threshold', prov:type='prov:Collection'])\n",
        "  hadMember(extent_threshold_collection, extent_threshold_param1)\n",
        "  hadMember(extent_threshold_collection, extent_threshold_param2)\n",
        "  entity(expected_voxels, [prov:type='spm:expected_voxels', prov:type='neurolex:VolumeInVoxels', prov:type='neurolex:voxel', prov:label=\"Expected_voxels_per_cluster\", prov:value=\"60.632000\" %% xsd:float])\n",
        "  entity(expected_clusters, [prov:type='spm:expected_clusters', prov:type='neurolex:cluster', prov:type='neurolex:voxel', prov:label=\"Expected_number_of_clusters\", prov:value=\"9.510000\" %% xsd:float])\n",
        "  entity(spm_results_metadata, [prov:type='spm:spm_results_metadata', prov:type='prov:Collection'])\n",
        "  hadMember(spm_results_metadata, height_threshold_collection)\n",
        "  hadMember(spm_results_metadata, extent_threshold_collection)\n",
        "  hadMember(spm_results_metadata, expected_voxels)\n",
        "  hadMember(spm_results_metadata, expected_clusters)\n",
        "endDocument\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Parsing SPM Table\n",
      "\n",
      "#declare dynamic arrays (lists) for each entity type\n",
      "set_level_id = []\n",
      "cluster_level_id=[]\n",
      "coords_id=[]\n",
      "peak_id=[]\n",
      "\n",
      "#declare collection entities\n",
      "set_level_collection_id=g.collection(\"set_level_collection\",[(prov.PROV['type'],spm_ns['set-level'])]).get_identifier()\n",
      "cluster_collection_id=g.collection(\"cluster_level_stats\",[(prov.PROV['type'],spm_ns['cluster-level'])]).get_identifier()\n",
      "coords_collection_id=g.collection(\"coordinates\",[(prov.PROV['type'],spm_ns['mm_mm_mm'])]).get_identifier()\n",
      "peak_collection_id=g.collection(\"peak_level_stats\",[(prov.PROV['type'],spm_ns['peak-level'])]).get_identifier()\n",
      "\n",
      "#indexes for each entity type\n",
      "set_level_indx=0\n",
      "cluster_level_indx=0\n",
      "coord_indx=0\n",
      "peak_indx=0\n",
      "\n",
      "\n",
      "#loop through spm_results_table\n",
      "for i in range(len(spm_results_table)):\n",
      "    \n",
      "    \n",
      "    #first make stat location entity so we can link all the statistics to the location\n",
      "    coords_id.append(g.entity(\"coord_\"+str(i), [(prov.PROV[\"type\"],neurolex_ns[\"coordinate\"]),\n",
      "                         (prov.PROV[\"type\"],nidm_ns[\"units\"]),\n",
      "                         (prov.PROV[\"label\"],\"mm_mm_mm\"),\n",
      "                         (prov.PROV[\"value\"],spm_results_table[i]['loc'])]).get_identifier())\n",
      "    g.hadMember(coords_collection_id,coords_id[coord_indx])\n",
      "    coord_indx=coord_indx+1\n",
      "    \n",
      "    \n",
      "    #for each row in the table\n",
      "    for j in spm_results_table[i]:\n",
      "        #print j\n",
      "        \n",
      "        #if we have a set_level_p stat and it's not empty, create appropriate entity\n",
      "        if (re.search(r\"\\bset_level_p\\b\",j, re.IGNORECASE) and (spm_results_table[i][j] != \"[]\")):\n",
      "            set_level_id.append(g.entity(\"set_level_p_\"+str(set_level_indx), [(prov.PROV[\"type\"],spm_ns[\"set-level\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                         (prov.PROV[\"label\"],\"set_level_p\"),\n",
      "                         (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier())\n",
      "            g.hadMember(set_level_collection_id,set_level_id[set_level_indx])\n",
      "            g.wasDerivedFrom(set_level_id[set_level_indx],coords_id[coord_indx-1])\n",
      "            #update index for set_level entity index\n",
      "            set_level_indx=set_level_indx+1\n",
      "    \n",
      "        #if we have a set_level_c stat and it's not emtpy, create appropriate entity\n",
      "        elif (re.search(r\"\\bset_level_c\\b\",j, re.IGNORECASE) and (spm_results_table[i][j] != \"[]\")):\n",
      "            set_level_id.append(g.entity(\"set_level_c_\"+str(set_level_indx), [(prov.PROV[\"type\"],spm_ns[\"set-level\"]),\n",
      "                         (prov.PROV[\"type\"],neurolex_ns[\"cluster\"]),\n",
      "                         (prov.PROV[\"label\"],\"set_level_c\"),\n",
      "                         (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier())\n",
      "            g.hadMember(set_level_collection_id,set_level_id[set_level_indx])\n",
      "            g.wasDerivedFrom(set_level_id[set_level_indx],coords_id[coord_indx-1])\n",
      "            #update index for set_level entity index\n",
      "            set_level_indx=set_level_indx+1\n",
      "        #if we have a FWE p-value stat, create appropriate entity\n",
      "        elif (re.search(r\"\\bcluster\\w+_FWEcorr_p\\b\",j, re.IGNORECASE)):\n",
      "            cluster_level_id.append(g.entity(\"cluster_\"+str(cluster_level_indx), [(prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"FWE\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier())\n",
      "            g.hadMember(cluster_collection_id,cluster_level_id[cluster_level_indx])\n",
      "            g.wasDerivedFrom( cluster_level_id[cluster_level_indx],coords_id[coord_indx-1])\n",
      "            #update cluster_level entity index\n",
      "            cluster_level_indx=cluster_level_indx+1\n",
      "        #if we have an FDR p-value stat, create approriate entity\n",
      "        elif (re.search(r\"\\bcluster\\w+FDRcorr_p\\b\",j, re.IGNORECASE)):\n",
      "            cluster_level_id.append(g.entity(\"cluster_\"+str(cluster_level_indx), [(prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"FDR\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier()) \n",
      "            g.hadMember(cluster_collection_id,cluster_level_id[cluster_level_indx])\n",
      "            g.wasDerivedFrom(cluster_level_id[cluster_level_indx], coords_id[coord_indx-1])\n",
      "            #update cluster_level entity index\n",
      "            cluster_level_indx=cluster_level_indx+1\n",
      "          \n",
      "        #if we have an Ke stat, create approriate entity\n",
      "        elif (re.search(r\"\\bcluster\\w+Ke\\b\",j, re.IGNORECASE)):\n",
      "            cluster_level_id.append(g.entity(\"cluster_\"+str(cluster_level_indx), [(prov.PROV[\"type\"],neurolex_ns[\"VolumeInVoxels\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"cluster-level\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier())\n",
      "            g.hadMember(cluster_collection_id,cluster_level_id[cluster_level_indx])\n",
      "            g.wasDerivedFrom(cluster_level_id[cluster_level_indx],coords_id[coord_indx-1])\n",
      "            #update cluster_level entity index\n",
      "            cluster_level_indx=cluster_level_indx+1\n",
      "         #if we have an uncorrected p-value stat, create approriate entity\n",
      "        elif (re.search(r\"\\bcluster\\w+Uncorr_p\\b\",j, re.IGNORECASE)):\n",
      "            cluster_level_id.append(g.entity(\"cluster_\"+str(cluster_level_indx), [(prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"uncorrected_p-value\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier() )\n",
      "            g.hadMember(cluster_collection_id,cluster_level_id[cluster_level_indx])\n",
      "            g.wasDerivedFrom(cluster_level_id[cluster_level_indx],coords_id[coord_indx-1])\n",
      "            #update cluster_level entity index\n",
      "            cluster_level_indx=cluster_level_indx+1\n",
      "        #if we have a FWE p-value stat at the peak level, create appropriate entity\n",
      "        elif (re.search(r\"\\bpeak\\w+_FWEcorr_p\\b\",j, re.IGNORECASE)):\n",
      "            peak_id.append(g.entity(\"peak_\"+str(peak_indx), [(prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"FWE\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier())\n",
      "            g.hadMember(peak_collection_id,peak_id[peak_indx])\n",
      "            g.wasDerivedFrom(peak_id[peak_indx],coords_id[coord_indx-1])\n",
      "            #update peak entity index\n",
      "            peak_indx=peak_indx+1\n",
      "        #if we have an FDR p-value stat, create approriate entity\n",
      "        elif (re.search(r\"\\bpeak\\w+FDRcorr_p\\b\",j, re.IGNORECASE)):\n",
      "            peak_id.append(g.entity(\"peak_\"+str(peak_indx), [(prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"FDR\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier()) \n",
      "            g.hadMember(peak_collection_id,peak_id[peak_indx])\n",
      "            g.wasDerivedFrom(peak_id[peak_indx],coords_id[coord_indx-1])\n",
      "            #update peak entity index\n",
      "            peak_indx=peak_indx+1\n",
      "        #if we have an T stat, create approriate entity\n",
      "        elif (re.search(r\"\\bpeak\\w+T\\b\",j, re.IGNORECASE)):\n",
      "            peak_id.append(g.entity(\"peak_\"+str(peak_indx), [(prov.PROV[\"type\"],neurolex_ns[\"t-statistic\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"peak-level\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier())\n",
      "            g.hadMember(peak_collection_id,peak_id[peak_indx])\n",
      "            peak_indx=peak_indx+1\n",
      "        #if we have an Z stat, create approriate entity\n",
      "        elif (re.search(r\"\\bpeak\\w+Z\\b\",j, re.IGNORECASE)):\n",
      "            peak_id.append(g.entity(\"peak_\"+str(peak_indx), [(prov.PROV[\"type\"],neurolex_ns[\"z-statistic\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"peak-level\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier())\n",
      "            g.hadMember(peak_collection_id,peak_id[peak_indx])\n",
      "            peak_indx=peak_indx+1\n",
      "    \n",
      "        #if we have an uncorrected p-value stat, create approriate entity\n",
      "        elif (re.search(r\"\\bpeak\\w+Uncorr_p\\b\",j, re.IGNORECASE)):\n",
      "            peak_id.append(g.entity(\"peak_\"+str(peak_indx), [(prov.PROV[\"type\"],neurolex_ns[\"p-value\"]),\n",
      "                             (prov.PROV[\"type\"],neurolex_ns[\"uncorrected_p-value\"]),\n",
      "                             (prov.PROV[\"label\"],j),\n",
      "                             (prov.PROV[\"value\"],spm_results_table[i][j])]).get_identifier()) \n",
      "            g.hadMember(peak_collection_id,peak_id[peak_indx])\n",
      "            peak_indx=peak_indx+1\n",
      "  \n",
      "                \n",
      "    \n",
      "print g.get_provn()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "unindent does not match any outer indentation level (<ipython-input-4-6ddf16da8c76>, line 130)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"spm_results.provn\",'w') as f:\n",
      "    f.write(g.get_provn())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}